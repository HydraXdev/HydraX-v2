#!/usr/bin/env python3
"""
WINLOSS REPORT - Truth Only
Standardized report for signal performance analysis
Uses broker outcomes for executed trades, shadow outcomes for non-executed
"""

import sys
import argparse
import sqlite3
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Tuple, Optional
from pathlib import Path

class WinLossReport:
    """Truth-only win/loss report with standardized output"""
    
    def __init__(self, db_path: str = "/root/HydraX-v2/bitten.db"):
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path)
        self.conn.row_factory = sqlite3.Row
        
    def run_report(self, 
                   from_time: str,
                   to_time: str,
                   population: str = "FIRED",
                   timeout_policy: str = "loss",
                   bins_type: str = "calibrated") -> None:
        """Generate and print the standardized winloss report"""
        
        # Convert times to unix timestamps
        from_ts = int(datetime.fromisoformat(from_time.replace('Z', '+00:00')).timestamp())
        to_ts = int(datetime.fromisoformat(to_time.replace('Z', '+00:00')).timestamp())
        
        # Print header
        print("=" * 80)
        print("WINLOSS REPORT - Truth Only")
        print("=" * 80)
        print(f"Period: {from_time} to {to_time}")
        print(f"Population: {population}")
        print(f"Timeout Policy: {timeout_policy}")
        print(f"Confidence Type: {bins_type}")
        print("=" * 80)
        print()
        
        # Check if unified_outcomes exists, create if not
        self._ensure_unified_view()
        
        # Get data
        signals = self._get_signals(from_ts, to_ts, population)
        
        if not signals:
            print("No signals found in specified timeframe")
            return
            
        # Filter out PENDING signals
        analyzed = [s for s in signals if s['outcome'] != 'PENDING']
        pending = [s for s in signals if s['outcome'] == 'PENDING']
        
        print(f"Total Signals: {len(signals)}")
        print(f"Analyzed: {len(analyzed)}")
        print(f"Pending: {len(pending)}")
        print()
        
        if not analyzed:
            print("No completed signals to analyze")
            return
        
        # Table A - Confidence Bins
        print("TABLE A - CONFIDENCE BINS")
        print("-" * 80)
        self._print_confidence_bins(analyzed, bins_type, timeout_policy)
        print()
        
        # Table B - Patterns
        print("TABLE B - PATTERNS")
        print("-" * 80)
        self._print_patterns(analyzed, timeout_policy)
        print()
        
        # Table C - Sessions  
        print("TABLE C - SESSIONS")
        print("-" * 80)
        self._print_sessions(analyzed, timeout_policy)
        print()
        
        # Top/Bottom performers
        print("TOP/BOTTOM PERFORMERS (min N=20)")
        print("-" * 80)
        self._print_top_bottom(analyzed, timeout_policy)
        print()
        
        # Footnotes
        print("FOOTNOTES:")
        print("-" * 80)
        print(f"- Timeout Policy: {timeout_policy.upper()} (timeouts counted as {'losses' if timeout_policy == 'loss' else 'excluded'})")
        print(f"- TRAIL/MANUAL: Counted by achieved_r (positive→TP, negative→SL)")
        print(f"- Excluded as PENDING: {len(pending)} signals")
        print(f"- Report generated: {datetime.now(timezone.utc).isoformat()}")
        
    def _ensure_unified_view(self):
        """Ensure unified_outcomes view exists"""
        try:
            # Check if view exists
            cursor = self.conn.execute("""
                SELECT name FROM sqlite_master 
                WHERE type='view' AND name='unified_outcomes'
            """)
            if not cursor.fetchone():
                print("[INFO] Unified view empty, building from existing tables...")
                # Create the view
                self.conn.execute("""
                    CREATE VIEW unified_outcomes AS
                    SELECT 
                        sa.signal_id,
                        sa.ts_fire,
                        sa.symbol,
                        sa.side,
                        sa.pattern,
                        sa.session,
                        sa.raw_confidence,
                        sa.calibrated_confidence,
                        sa.risk_reward,
                        COALESCE(
                            sov.outcome,
                            sos.outcome_shadow,
                            CASE 
                                WHEN strftime('%s', 'now') - sa.ts_fire > 7200 THEN 'TIMEOUT'
                                ELSE 'PENDING'
                            END
                        ) as outcome,
                        COALESCE(sov.achieved_r, sos.achieved_r_shadow, 0) as achieved_r,
                        COALESCE(sov.duration_min, sos.duration_min_shadow, 
                                 CAST((strftime('%s', 'now') - sa.ts_fire) / 60 AS INTEGER)) as duration_min
                    FROM signals_all sa
                    LEFT JOIN signal_outcomes_verified sov ON sa.signal_id = sov.signal_id
                    LEFT JOIN signal_outcomes_shadow sos ON sa.signal_id = sos.signal_id
                    WHERE sa.decision = 'FIRED'
                """)
                print("[INFO] Unified view created successfully")
        except sqlite3.OperationalError:
            # View already exists or other error
            pass
    
    def _get_signals(self, from_ts: int, to_ts: int, population: str) -> List[Dict]:
        """Get signals from unified view"""
        
        query = """
        SELECT 
            signal_id,
            ts_fire,
            symbol,
            side,
            pattern,
            session,
            raw_confidence,
            calibrated_confidence,
            risk_reward,
            outcome,
            achieved_r,
            duration_min
        FROM unified_outcomes
        WHERE ts_fire BETWEEN ? AND ?
        """
        
        if population != "ALL":
            # Already filtered to FIRED in the view
            pass
            
        cursor = self.conn.execute(query, (from_ts, to_ts))
        return [dict(row) for row in cursor]
    
    def _calculate_metrics(self, signals: List[Dict], timeout_policy: str) -> Dict:
        """Calculate standard metrics for a group of signals"""
        
        if not signals:
            return {
                'N': 0, 'TP': 0, 'SL': 0, 'TIMEOUT': 0,
                'Win%': 0.0, 'Expectancy': 0.0, 'Median_Time': 0
            }
        
        n = len(signals)
        
        # Count outcomes - map TRAIL/MANUAL based on achieved_r
        tp = sum(1 for s in signals if 
                s['outcome'] == 'TP' or 
                (s['outcome'] in ['TRAIL', 'TRAIL_CLOSE', 'MANUAL', 'MANUAL_CLOSE'] and s['achieved_r'] > 0))
        
        sl = sum(1 for s in signals if 
                s['outcome'] == 'SL' or 
                s['outcome'] == 'STOP_OUT' or
                (s['outcome'] in ['TRAIL', 'TRAIL_CLOSE', 'MANUAL', 'MANUAL_CLOSE'] and s['achieved_r'] <= 0))
        
        timeout = sum(1 for s in signals if s['outcome'] == 'TIMEOUT')
        
        # Calculate win% based on timeout policy
        if timeout_policy == 'loss':
            denominator = tp + sl + timeout
        else:  # exclude
            denominator = tp + sl
            
        win_pct = (tp / denominator * 100) if denominator > 0 else 0.0
        
        # Expectancy (average achieved R)
        r_values = [s['achieved_r'] for s in signals if s['achieved_r'] is not None]
        expectancy = sum(r_values) / len(r_values) if r_values else 0.0
        
        # Median time (simple average since SQLite doesn't have median)
        times = [s['duration_min'] for s in signals if s['duration_min'] is not None]
        median_time = sum(times) / len(times) if times else 0
        
        return {
            'N': n,
            'TP': tp,
            'SL': sl,
            'TIMEOUT': timeout,
            'Win%': win_pct,
            'Expectancy': expectancy,
            'Median_Time': median_time
        }
    
    def _print_confidence_bins(self, signals: List[Dict], bins_type: str, timeout_policy: str):
        """Print confidence bins table"""
        
        # Define bins
        bins = [
            ('70-75', 70, 75),
            ('75-80', 75, 80),
            ('80-85', 80, 85),
            ('85-90', 85, 90),
            ('90-95', 90, 95),
            ('95+', 95, 100)
        ]
        
        # Choose confidence field
        conf_field = 'calibrated_confidence' if bins_type == 'calibrated' else 'raw_confidence'
        
        # Header
        print(f"{'Bin':<10} {'N':>6} {'TP':>6} {'SL':>6} {'TO':>6} {'Win%':>8} {'Exp(R)':>8} {'Med(m)':>8} {'Cov%':>8}")
        print("-" * 80)
        
        total_n = len(signals)
        
        for bin_name, min_conf, max_conf in bins:
            if bin_name == '95+':
                bin_signals = [s for s in signals if s[conf_field] and s[conf_field] >= min_conf]
            else:
                bin_signals = [s for s in signals if s[conf_field] and min_conf <= s[conf_field] < max_conf]
            
            metrics = self._calculate_metrics(bin_signals, timeout_policy)
            coverage = (metrics['N'] / total_n * 100) if total_n > 0 else 0
            
            print(f"{bin_name:<10} {metrics['N']:>6} {metrics['TP']:>6} {metrics['SL']:>6} "
                  f"{metrics['TIMEOUT']:>6} {metrics['Win%']:>7.1f}% {metrics['Expectancy']:>8.2f} "
                  f"{metrics['Median_Time']:>7.0f}m {coverage:>7.1f}%")
    
    def _print_patterns(self, signals: List[Dict], timeout_policy: str):
        """Print patterns table"""
        
        # Group by pattern
        patterns = {}
        for s in signals:
            pattern = s['pattern'] or 'UNKNOWN'
            if pattern not in patterns:
                patterns[pattern] = []
            patterns[pattern].append(s)
        
        # Header
        print(f"{'Pattern':<30} {'N':>6} {'Win%':>8} {'Exp(R)':>8} {'Med(m)':>8}")
        print("-" * 60)
        
        # Sort by N descending
        for pattern, pattern_signals in sorted(patterns.items(), key=lambda x: len(x[1]), reverse=True):
            metrics = self._calculate_metrics(pattern_signals, timeout_policy)
            print(f"{pattern:<30} {metrics['N']:>6} {metrics['Win%']:>7.1f}% "
                  f"{metrics['Expectancy']:>8.2f} {metrics['Median_Time']:>7.0f}m")
    
    def _print_sessions(self, signals: List[Dict], timeout_policy: str):
        """Print sessions table"""
        
        # Group by session
        sessions = {}
        for s in signals:
            session = s['session'] or 'UNKNOWN'
            if session not in sessions:
                sessions[session] = []
            sessions[session].append(s)
        
        # Header
        print(f"{'Session':<15} {'N':>6} {'Win%':>8} {'Exp(R)':>8} {'Med(m)':>8}")
        print("-" * 50)
        
        # Sort by predefined order
        session_order = ['ASIAN', 'LONDON', 'NY', 'OVERLAP', 'UNKNOWN']
        
        for session in session_order:
            if session in sessions:
                metrics = self._calculate_metrics(sessions[session], timeout_policy)
                print(f"{session:<15} {metrics['N']:>6} {metrics['Win%']:>7.1f}% "
                      f"{metrics['Expectancy']:>8.2f} {metrics['Median_Time']:>7.0f}m")
    
    def _print_top_bottom(self, signals: List[Dict], timeout_policy: str):
        """Print top/bottom performers by expectancy"""
        
        # Group by pattern+session combos
        buckets = {}
        for s in signals:
            bucket_key = f"{s['pattern'] or 'UNK'}_{s['session'] or 'UNK'}"
            if bucket_key not in buckets:
                buckets[bucket_key] = []
            buckets[bucket_key].append(s)
        
        # Filter to min N=20 and calculate metrics
        qualified_buckets = []
        for bucket_key, bucket_signals in buckets.items():
            if len(bucket_signals) >= 20:
                metrics = self._calculate_metrics(bucket_signals, timeout_policy)
                qualified_buckets.append((bucket_key, metrics))
        
        if not qualified_buckets:
            print("No buckets with N≥20 signals")
            return
        
        # Sort by expectancy
        qualified_buckets.sort(key=lambda x: x[1]['Expectancy'], reverse=True)
        
        # Top 3
        print("TOP 3 PERFORMERS:")
        for i, (bucket, metrics) in enumerate(qualified_buckets[:3], 1):
            print(f"{i}. {bucket}: N={metrics['N']}, Win%={metrics['Win%']:.1f}%, Exp={metrics['Expectancy']:.2f}")
        
        print()
        
        # Bottom 3
        print("BOTTOM 3 PERFORMERS:")
        for i, (bucket, metrics) in enumerate(qualified_buckets[-3:], 1):
            print(f"{i}. {bucket}: N={metrics['N']}, Win%={metrics['Win%']:.1f}%, Exp={metrics['Expectancy']:.2f}")
    
    def close(self):
        """Close database connection"""
        self.conn.close()


def main():
    """CLI entry point"""
    
    parser = argparse.ArgumentParser(description='WINLOSS Report - Truth Only')
    
    # Time aliases
    parser.add_argument('alias', nargs='?', 
                       choices=['last24h', 'today', 'yesterday', 'last7d'],
                       help='Time period alias')
    
    # Custom time range
    parser.add_argument('--from', dest='from_time', type=str,
                       help='Start time (UTC ISO format)')
    parser.add_argument('--to', dest='to_time', type=str,
                       help='End time (UTC ISO format)')
    
    # Options
    parser.add_argument('--population', choices=['FIRED', 'ALL'], 
                       default='FIRED',
                       help='Population to analyze (default: FIRED)')
    parser.add_argument('--timeout_policy', choices=['loss', 'exclude'],
                       default='loss',
                       help='How to count timeouts (default: loss)')
    parser.add_argument('--bins', choices=['calibrated', 'raw'],
                       default='calibrated', 
                       help='Confidence type for binning (default: calibrated)')
    
    args = parser.parse_args()
    
    # Determine time range
    now = datetime.now(timezone.utc)
    
    if args.alias:
        if args.alias == 'last24h':
            from_time = (now - timedelta(hours=24)).isoformat()
            to_time = now.isoformat()
        elif args.alias == 'today':
            from_time = now.replace(hour=0, minute=0, second=0, microsecond=0).isoformat()
            to_time = now.isoformat()
        elif args.alias == 'yesterday':
            yesterday = now - timedelta(days=1)
            from_time = yesterday.replace(hour=0, minute=0, second=0, microsecond=0).isoformat()
            to_time = yesterday.replace(hour=23, minute=59, second=59, microsecond=999999).isoformat()
        elif args.alias == 'last7d':
            from_time = (now - timedelta(days=7)).isoformat()
            to_time = now.isoformat()
    elif args.from_time and args.to_time:
        from_time = args.from_time
        to_time = args.to_time
    else:
        # Default to last 24h
        from_time = (now - timedelta(hours=24)).isoformat()
        to_time = now.isoformat()
    
    # Run report
    report = WinLossReport()
    try:
        report.run_report(
            from_time=from_time,
            to_time=to_time,
            population=args.population,
            timeout_policy=args.timeout_policy,
            bins_type=args.bins
        )
    finally:
        report.close()


if __name__ == '__main__':
    main()